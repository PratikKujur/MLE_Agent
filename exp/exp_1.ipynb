{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d524a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    trace,\n",
    "    function_tool,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    input_guardrail,\n",
    "    GuardrailFunctionOutput,\n",
    ")\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4a8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_url=\"https://api.groq.com/openai/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d8c71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_attach_logprobs_to_output',\n",
       " '_fetch_response',\n",
       " '_get_client',\n",
       " '_merge_headers',\n",
       " '_non_null_or_omit',\n",
       " 'get_response',\n",
       " 'stream_response']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(OpenAIChatCompletionsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41b4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions=\"Your are a AI teacher at a MIT University, you have to solve the doubths of your students\"\n",
    "groq_model = OpenAIChatCompletionsModel(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    openai_client=AsyncOpenAI(\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d88b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_teacher=Agent(name=\"AI_Teacher\",instructions=instructions,model=groq_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c916886",
   "metadata": {},
   "outputs": [],
   "source": [
    "description=\"Answers the question about ai\"\n",
    "tool_1=agent_teacher.as_tool(tool_name=\"AI Teacher\",tool_description=description,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce218ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[tool_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8fb2138",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'code=400, message=tool `AI Teacher` cannot contain whitespace in its name, type=invalid_request_error', 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m message=\u001b[33m\"\u001b[39m\u001b[33mhow llm understand language\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[33m\"\u001b[39m\u001b[33mAutomated SDR\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     result= \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(ai_teacher,message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\run.py:372\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    325\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    371\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    373\u001b[39m     starting_agent,\n\u001b[32m    374\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    375\u001b[39m     context=context,\n\u001b[32m    376\u001b[39m     max_turns=max_turns,\n\u001b[32m    377\u001b[39m     hooks=hooks,\n\u001b[32m    378\u001b[39m     run_config=run_config,\n\u001b[32m    379\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    380\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    381\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    382\u001b[39m     session=session,\n\u001b[32m    383\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\run.py:671\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    664\u001b[39m         starting_agent,\n\u001b[32m    665\u001b[39m         sequential_guardrails,\n\u001b[32m    666\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    667\u001b[39m         context_wrapper,\n\u001b[32m    668\u001b[39m     )\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    672\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    673\u001b[39m         starting_agent,\n\u001b[32m    674\u001b[39m         parallel_guardrails,\n\u001b[32m    675\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    676\u001b[39m         context_wrapper,\n\u001b[32m    677\u001b[39m     ),\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    679\u001b[39m         agent=current_agent,\n\u001b[32m    680\u001b[39m         all_tools=all_tools,\n\u001b[32m    681\u001b[39m         original_input=original_input,\n\u001b[32m    682\u001b[39m         generated_items=generated_items,\n\u001b[32m    683\u001b[39m         hooks=hooks,\n\u001b[32m    684\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    685\u001b[39m         run_config=run_config,\n\u001b[32m    686\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    687\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    688\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    689\u001b[39m     ),\n\u001b[32m    690\u001b[39m )\n\u001b[32m    692\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    693\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\run.py:1689\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1690\u001b[39m     agent,\n\u001b[32m   1691\u001b[39m     system_prompt,\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1693\u001b[39m     output_schema,\n\u001b[32m   1694\u001b[39m     all_tools,\n\u001b[32m   1695\u001b[39m     handoffs,\n\u001b[32m   1696\u001b[39m     hooks,\n\u001b[32m   1697\u001b[39m     context_wrapper,\n\u001b[32m   1698\u001b[39m     run_config,\n\u001b[32m   1699\u001b[39m     tool_use_tracker,\n\u001b[32m   1700\u001b[39m     server_conversation_tracker,\n\u001b[32m   1701\u001b[39m     prompt_config,\n\u001b[32m   1702\u001b[39m )\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1705\u001b[39m     agent=agent,\n\u001b[32m   1706\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1715\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1716\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\run.py:1952\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1942\u001b[39m previous_response_id = (\n\u001b[32m   1943\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1945\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1946\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1947\u001b[39m )\n\u001b[32m   1948\u001b[39m conversation_id = (\n\u001b[32m   1949\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1950\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1953\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1954\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1955\u001b[39m     model_settings=model_settings,\n\u001b[32m   1956\u001b[39m     tools=all_tools,\n\u001b[32m   1957\u001b[39m     output_schema=output_schema,\n\u001b[32m   1958\u001b[39m     handoffs=handoffs,\n\u001b[32m   1959\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1960\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1961\u001b[39m     ),\n\u001b[32m   1962\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1963\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1964\u001b[39m     prompt=prompt_config,\n\u001b[32m   1965\u001b[39m )\n\u001b[32m   1967\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1969\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:73\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     57\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     prompt: ResponsePromptParam | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     67\u001b[39m ) -> ModelResponse:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     69\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     70\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     71\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     72\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     74\u001b[39m             system_instructions,\n\u001b[32m     75\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     76\u001b[39m             model_settings,\n\u001b[32m     77\u001b[39m             tools,\n\u001b[32m     78\u001b[39m             output_schema,\n\u001b[32m     79\u001b[39m             handoffs,\n\u001b[32m     80\u001b[39m             span_generation,\n\u001b[32m     81\u001b[39m             tracing,\n\u001b[32m     82\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     83\u001b[39m             prompt=prompt,\n\u001b[32m     84\u001b[39m         )\n\u001b[32m     86\u001b[39m         message: ChatCompletionMessage | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     87\u001b[39m         first_choice: Choice | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:321\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    315\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    316\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    317\u001b[39m )\n\u001b[32m    319\u001b[39m stream_param: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m omit\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    322\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    323\u001b[39m     messages=converted_messages,\n\u001b[32m    324\u001b[39m     tools=tools_param,\n\u001b[32m    325\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.temperature),\n\u001b[32m    326\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_p),\n\u001b[32m    327\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.frequency_penalty),\n\u001b[32m    328\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.presence_penalty),\n\u001b[32m    329\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.max_tokens),\n\u001b[32m    330\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    331\u001b[39m     response_format=response_format,\n\u001b[32m    332\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    333\u001b[39m     stream=cast(Any, stream_param),\n\u001b[32m    334\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(stream_options),\n\u001b[32m    335\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(store),\n\u001b[32m    336\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(reasoning_effort),\n\u001b[32m    337\u001b[39m     verbosity=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.verbosity),\n\u001b[32m    338\u001b[39m     top_logprobs=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_logprobs),\n\u001b[32m    339\u001b[39m     prompt_cache_retention=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001b[32m    340\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    341\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    342\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    343\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.metadata),\n\u001b[32m    344\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    345\u001b[39m )\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2675\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2676\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2677\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m             {\n\u001b[32m   2682\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m             },\n\u001b[32m   2718\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m         ),\n\u001b[32m   2722\u001b[39m         options=make_request_options(\n\u001b[32m   2723\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m         ),\n\u001b[32m   2725\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLE_Agent\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1594\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1596\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'code=400, message=tool `AI Teacher` cannot contain whitespace in its name, type=invalid_request_error', 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "instructions=\"Your are a AI teacher at a MIT University, you have to solve the doubths of your students\"\n",
    "\n",
    "ai_teacher=Agent(\n",
    "    name=\"AI_teacher\",\n",
    "    instructions=instructions,\n",
    "    tools=tools,\n",
    "    model=groq_model\n",
    ")\n",
    "\n",
    "message=\"how llm understand language\"\n",
    "\n",
    "with trace(\"Automated SDR\"):\n",
    "    result= await Runner.run(ai_teacher,message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e1b2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "data,target=datasets.load_iris(return_X_y=True,as_frame=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "166f1c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target']=target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34dc1415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06648750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sepal length (cm)': np.int64(0),\n",
       " 'sepal width (cm)': np.int64(0),\n",
       " 'petal length (cm)': np.int64(0),\n",
       " 'petal width (cm)': np.int64(0),\n",
       " 'target': np.int64(0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_vals=data.isnull().sum()\n",
    "dict(missing_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a16ab66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sepal length (cm)': dtype('float64'),\n",
       " 'sepal width (cm)': dtype('float64'),\n",
       " 'petal length (cm)': dtype('float64'),\n",
       " 'petal width (cm)': dtype('float64'),\n",
       " 'target': dtype('int64')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9395a46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efb1f63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sepal length (cm)'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a0a5580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c18e8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('D:/Projects/MLE_Agent/exp/flower_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
